{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1da2488",
   "metadata": {},
   "source": [
    "# Mathematical intuition of gradient descent\n",
    "> Understanding and coding gradient descent from scratch\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Gieun Kwak\n",
    "- categories: [optimization, gradient descent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c436bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sympy import *\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# import matplotlib_inline.backend_inline\n",
    "# matplotlib_inline.backend_inline.set_matplotlib_formats('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd335b9",
   "metadata": {},
   "source": [
    "### 1. How gradient descent looks like\n",
    "\n",
    "Gradient descent is commonly used in ML and AI. Looking at the image below, we encounter from lectures that the gradient descent is a 'process of finding the most lowest point of the function'. While this is a very concise and accurate explanation of gradient descent, understanding it in a mathematical way would enable a deeper understanding.\n",
    "\n",
    "![](my_icons/03_/gradient-descent-example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae0136",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0228c",
   "metadata": {},
   "source": [
    "### 2. How gradient descent works: simple maths\n",
    "\n",
    "The gradient descent image above goes through a line until it reaches the 'bottom' of the function. This 'bottom' of the function is called 'minimum'. We want to reach the minimum because it will give us the best values for our problem. One of the common use of gradient is when we estimate the parameters of a ML or AI model. We want to find the paramaters that has the smalles loss value (closes to the actual observation).\n",
    "\n",
    "Lets consider a function $y = (x-5)^2$. We want to find the minimum value of this function by following a sequence of steps.\n",
    "1. Start from a certain point\n",
    "2. Move to a new point that gives a better objective value (here, smaller values)\n",
    "3. Repeat step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4419102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "# x = np.arange(2, 8, 0.01)\n",
    "# def y(x):\n",
    "#     return (x-5)**2\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, y(x))\n",
    "# ax.set_xlabel(\"x\")\n",
    "# ax.set_ylabel(\"y\")\n",
    "# ax.plot([5], y(5), marker='o', color='black')\n",
    "\n",
    "# fig.savefig('my_icons/03_/one.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67ed09",
   "metadata": {},
   "source": [
    "![](my_icons/03_/one.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de0b5a",
   "metadata": {},
   "source": [
    "One thing most explanations exclude is the constraint. Actually, for every optimization problem, there is an objective function and a constraint. The objective function is $y = (x-5)^2$. And the constraint is hidden, which is $x \\in R$, $y \\in R$ ($R$ means real numbers).\n",
    "\n",
    "The optimization problem changes along the constraints, but for now lets say the constraint is $y \\geq (x-5)^2$. This means the set of potential solutions for this problem is all the points above the function line.\n",
    "\n",
    "The process of gradient descent will only happen inside the blue area (points that satisfy the constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50009d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "# x = np.arange(2, 8, 0.01)\n",
    "# def y(x):\n",
    "#     return (x-5)**2\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, y(x))\n",
    "# ax.set_xlabel(\"x\")\n",
    "# ax.set_ylabel(\"y\")\n",
    "\n",
    "# ax.fill_between(x, y(x), 9, color='blue', alpha=.1)\n",
    "# ax.plot([5], y(5), marker='o', color='black')\n",
    "\n",
    "# fig.savefig('my_icons/03_/two.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405b038",
   "metadata": {},
   "source": [
    "![](my_icons/03_/two.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45093137",
   "metadata": {},
   "source": [
    "Now lets say we start at a point where $x=7$ and $y=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77add1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "# x = np.arange(2, 8, 0.01)\n",
    "# def y(x):\n",
    "#     return (x-5)**2\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, y(x))\n",
    "# ax.set_xlabel(\"x\")\n",
    "# ax.set_ylabel(\"y\")\n",
    "# ax.fill_between(x, y(x), 9, color='blue', alpha=.1)\n",
    "\n",
    "# ax.plot([7], y(7), marker='o', color='red')\n",
    "# ax.plot([5], y(5), marker='o', color='black')\n",
    "\n",
    "# fig.savefig('my_icons/03_/three.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f981472",
   "metadata": {},
   "source": [
    "![](my_icons/03_/three.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be28093",
   "metadata": {},
   "source": [
    "This red dot can go anywhere as long as it is in the blue area. Lets say this dot moves to $(6,6)$. This point is still in the blue area and satisfies the constraint we have set. This is called feasibility.\n",
    "\n",
    "- __The process of gradient descent ONLY considers the solutions that are feasible (satisfies constraints)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05110095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "# x = np.arange(2, 8, 0.01)\n",
    "# def y(x):\n",
    "#     return (x-5)**2\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, y(x))\n",
    "# ax.set_xlabel(\"x\")\n",
    "# ax.set_ylabel(\"y\")\n",
    "# ax.fill_between(x, y(x), 9, color='blue', alpha=.1)\n",
    "\n",
    "# ax.plot([7], y(7), marker='o', color='red')\n",
    "# ax.plot([6], 6, marker='o', color='green')\n",
    "# ax.plot([6], 2, marker='o', color='blue')\n",
    "# ax.plot([5], y(5), marker='o', color='black')\n",
    "\n",
    "# ax.arrow(7, y(7), 6-7+0.1, 2-y(7)+0.1, head_width=0.1, head_length=0.1);\n",
    "\n",
    "# fig.savefig('my_icons/03_/four.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a3203",
   "metadata": {},
   "source": [
    "![](my_icons/03_/four.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7d875",
   "metadata": {},
   "source": [
    "While the point $(6,6)$ is feasible, does it go to the optimal value? The optimal value we want to find is the smallest point of $y=(x-5)^2$.\n",
    "\n",
    "Y value when x=7 : 4 <br>\n",
    "Y value when x=6 : 6\n",
    "\n",
    "The value of the function rather increased, and it means that this is not the right direction.\n",
    "\n",
    "If the red dot goes to the direction of blue dot, the objective function decreases. This then would be the right direction we want to go. And the 'amount' we want to move is the learning rate($\\lambda$) which is a small step towards the direction.\n",
    "\n",
    "- __The solution for each step is updated by $x^1 = x^0 + \\lambda d^0$, and the direction is where the optimal value gets closer to the global minimum.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb666b7",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac32b4",
   "metadata": {},
   "source": [
    "### 3. How is gradient calculated?\n",
    "\n",
    "There are two aspects in calculating gradient. First is the 'direction', and the second is the 'distance'. <br> Lets say $d$ is for firection and $\\alpha$ for step size ('distance'). Then this could be formulated as the following: $f(x^k + \\alpha d^k) < f(x^k)$, which means the new value is smaller than the current function value. <br>\n",
    "Using Taylor's expanson, it could be formulated as $f(x^k + \\alpha d^k) \\approx f(x^k) + \\alpha\\nabla f(x^k)^T d_k$ <br>\n",
    "Since $f(x^k + \\alpha d^k) < f(x^k)$, we want $\\alpha\\nabla f(x^k)^T d_k$ to be smaller than zero. The steepest direction $d_k$ would be $-\\nabla f(x^k)$ since it is the opposite direction from $\\nabla f(x^k)^T$\n",
    "\n",
    "Then, the new point is updated every step by $x^{k+1} \\leftarrow x_k - \\alpha \\nabla f(x_k)$\n",
    "\n",
    "This process will continue until $||\\nabla f(x_k)|| \\leq \\epsilon$ where $\\epsilon$ is a very small number. This is the point where the gradient is small enough or vanishes. This means that there is no more 'movement'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5954dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "# x = np.arange(2, 8, 0.01)\n",
    "# def y(x):\n",
    "#     return (x-5)**2\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(x, y(x))\n",
    "# ax.set_xlabel(\"x\")\n",
    "# ax.set_ylabel(\"y\")\n",
    "# ax.fill_between(x, y(x), 9, color='blue', alpha=.1)\n",
    "\n",
    "# ax.plot([7], y(7), marker='o', color='red')\n",
    "# ax.plot([5], y(5), marker='o', color='black')\n",
    "\n",
    "# def line1(x):\n",
    "#     return 4*x - 24\n",
    "# def line2(x):\n",
    "#     return x*0\n",
    "\n",
    "# ax.plot(np.arange(6, 8, 0.01), line1(np.arange(6, 8, 0.01)), color='red')\n",
    "# ax.plot(x, line2(x), color='black');\n",
    "# fig.savefig('my_icons/03_/five.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f578c",
   "metadata": {},
   "source": [
    "![](my_icons/03_/five.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed127b5",
   "metadata": {},
   "source": [
    "The red dot has a negative gradient, and it moves the point towards left. If the point reaches the black dot, the gradient is zero (vanishes), meaning there is no further direction for the point to move, indicating the end of the search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb9ede",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29a42f",
   "metadata": {},
   "source": [
    "### 4. Newton's method from scratch\n",
    "\n",
    "One of the methods of gradient descent is Newton's method.\n",
    "\n",
    "Let $x^k$ be the current state and consider a second order Taylor approximation of $f(x)$. <br>\n",
    "$g(x) = f(x^k)+\\nabla f(x^k)^T (x-x^k) + \\frac{1}{2}(x-x^k)\\nabla^2f(x^k)(x-x^k)$\n",
    "\n",
    "The next step would be to find the point that minimizes the function $g(x)$. That point would be where the first derivative equals to zero.\n",
    "$\\nabla g(x) = \\nabla f(x^k)+\\nabla^2f(x^k)(x-x^k) = 0$\n",
    "\n",
    "Solving this equation for $x$, this gives us <br>\n",
    "$x = x^k - [\\nabla^2f(x_k)]^{-1} \\nabla f(x^k)$ which will be the new $x^{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a554d",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f13ac9",
   "metadata": {},
   "source": [
    "To formulate this into the code, here is the psuedo code.\n",
    "\n",
    "![](my_icons/03_/newtons_method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43de64",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "- $k$ is the indicator for current step \n",
    "- $\\epsilon$ is the minimum value for stopping the search\n",
    "- $\\alpha$ is the step size. $\\alpha$ changes via line search. This means that we want to find a step size that actually leads to minimizing the function value\n",
    "- $\\rho$ and $c$ are scalars used to derive $\\alpha$ during line search. They are between 0 and 1\n",
    "\n",
    "#### Code walkthrough\n",
    "\n",
    "- Set the initial values for $\\alpha$, $d$, $k$. Initial starting point $x^0$ will be provided via input\n",
    "- The search ends only if the first derivative of $f(x)$ is smaller than $\\epsilon$. Here, it is actually the norm value of the vector.\n",
    "- Inside the search, first determine perform line search to find $\\alpha$\n",
    "    - If $\\alpha$ is found, update $x^{k+1}$ as well as $d^{k+1}$\n",
    "- Continue until vanishing gradient or when it is small enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7618f",
   "metadata": {},
   "source": [
    "#### Coding - from scratch\n",
    "\n",
    "Lets solve the optimzal minimum value for the following function.\n",
    "$$y = 100(x_2-x_1^2)^2 + (1-x_1)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8629bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "\n",
    "def plot_contour_2d(X1, X2, y):\n",
    "    fig, ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "    c_plot = ax.contour(X1, X2, y)\n",
    "    ax.clabel(c_plot, inline=True, \n",
    "              fontsize=10);\n",
    "    ax.set_title('2D contour plot');\n",
    "    \n",
    "def plot_contour_3d(X1, X2, y, save_dir):\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.contour3D(X1, X2, y, 50, cmap='binary')\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_zlabel(\"y\")\n",
    "    ax.set_title('3D contour plot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc6c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "# X1 = np.arange(-50, 50, 0.1)\n",
    "# X2 = np.arange(-50, 20, 0.1)\n",
    "# X1, X2 = np.meshgrid(X1, X2)\n",
    "# y = 100 * (X2 - X1**2)**2 + (1-X1)**2\n",
    "\n",
    "# plot_contour_3d(X1, X2, y, 'my_icons/03_/3D_contour.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd58617",
   "metadata": {},
   "source": [
    "![](my_icons/03_/3D_contour.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f81237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "# define x and y\n",
    "x1 = Symbol('x1')\n",
    "x2 = Symbol('x2')\n",
    "y = 100 * (x2 - x1**2)**2 + (1-x1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0f2b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Y\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(1 - x_{1}\\right)^{2} + 100 \\left(- x_{1}^{2} + x_{2}\\right)^{2}$"
      ],
      "text/plain": [
       "(1 - x1)**2 + 100*(-x1**2 + x2)**2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collapse\n",
    "\n",
    "print(\"Function Y\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "805e4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse\n",
    "\n",
    "# in order for the function to be calculated with x input, it has to be 'lambdafied'\n",
    "# for more information, see the package sympy\n",
    "# it allows for place holders for the function, making it easy for us to read and understand\n",
    "\n",
    "def f(x1_value, x2_value, x1=x1, x2=x2, function=y):\n",
    "    \"\"\"\n",
    "    Input the values for x1 and x2, to the function\n",
    "    \"\"\"\n",
    "    working_function = lambdify((x1,x2), function, 'numpy')\n",
    "    return working_function(x1_value, x2_value)\n",
    "\n",
    "def hessian_matrix(x1_value, x2_value, x1, x2, y):\n",
    "    \"\"\"\n",
    "    Calculates the hessian matrix for 2 variable function.\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((2,2))\n",
    "    for i, der1 in enumerate([x1,x2]):\n",
    "        for j, der2 in enumerate([x1,x2]):\n",
    "            matrix[i,j] = f(x1_value, x2_value, x1, x2, y.diff(der1).diff(der2))\n",
    "    return matrix\n",
    "\n",
    "def first_order_matrix(x1_value, x2_value, x1, x2, y):\n",
    "    matrix = np.zeros((2,1))\n",
    "    for i, der in enumerate([x1,x2]):\n",
    "        matrix[i] = f(x1_value, x2_value, x1, x2, y.diff(der))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124564fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative x1\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 400 x_{1} \\left(- x_{1}^{2} + x_{2}\\right) + 2 x_{1} - 2$"
      ],
      "text/plain": [
       "-400*x1*(-x1**2 + x2) + 2*x1 - 2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collapse\n",
    "\n",
    "print(\"Partial derivative x1\")\n",
    "y.diff(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35384502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative x2\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 200 x_{1}^{2} + 200 x_{2}$"
      ],
      "text/plain": [
       "-200*x1**2 + 200*x2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collapse\n",
    "\n",
    "print(\"Partial derivative x2\")\n",
    "y.diff(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643e007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative x1, x2\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 400 x_{1}$"
      ],
      "text/plain": [
       "-400*x1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collapse\n",
    "\n",
    "print(\"Partial derivative x1, x2\")\n",
    "y.diff(x1).diff(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d2d1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative x2, x1\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 400 x_{1}$"
      ],
      "text/plain": [
       "-400*x1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collapse\n",
    "\n",
    "print(\"Partial derivative x2, x1\")\n",
    "y.diff(x2).diff(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566de1a",
   "metadata": {},
   "source": [
    "Newton's method implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6d367-c1a4-47ab-99a1-9cee327982b5",
   "metadata": {},
   "source": [
    "---\n",
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scratch] *",
   "language": "python",
   "name": "conda-env-scratch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
