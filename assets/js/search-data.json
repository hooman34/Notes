{
  
    
        "post0": {
            "title": "Mathematical intuition of gradient descent",
            "content": "import matplotlib.pyplot as plt import numpy as np from sympy import * # from IPython.display import set_matplotlib_formats # import matplotlib_inline.backend_inline # matplotlib_inline.backend_inline.set_matplotlib_formats(&#39;png&#39;) . . 1. How gradient descent looks like . Gradient descent is commonly used in ML and AI. Looking at the image below, we encounter from lectures that the gradient descent is a &#39;process of finding the most lowest point of the function&#39;. While this is a very concise and accurate explanation of gradient descent, understanding it in a mathematical way would enable a deeper understanding. . . . 2. How gradient descent works: simple maths . The gradient descent image above goes through a line until it reaches the &#39;bottom&#39; of the function. This &#39;bottom&#39; of the function is called &#39;minimum&#39;. We want to reach the minimum because it will give us the best values for our problem. One of the common use of gradient is when we estimate the parameters of a ML or AI model. We want to find the paramaters that has the smalles loss value (closes to the actual observation). . Lets consider a function $y = (x-5)^2$. We want to find the minimum value of this function by following a sequence of steps. . Start from a certain point | Move to a new point that gives a better objective value (here, smaller values) | Repeat step 2 | # x = np.arange(2, 8, 0.01) # def y(x): # return (x-5)**2 # fig, ax = plt.subplots() # ax.plot(x, y(x)) # ax.set_xlabel(&quot;x&quot;) # ax.set_ylabel(&quot;y&quot;) # ax.plot([5], y(5), marker=&#39;o&#39;, color=&#39;black&#39;) # fig.savefig(&#39;my_icons/03_/one.png&#39;); . . . One thing most explanations exclude is the constraint. Actually, for every optimization problem, there is an objective function and a constraint. The objective function is $y = (x-5)^2$. And the constraint is hidden, which is $x in R$, $y in R$ ($R$ means real numbers). . The optimization problem changes along the constraints, but for now lets say the constraint is $y geq (x-5)^2$. This means the set of potential solutions for this problem is all the points above the function line. . The process of gradient descent will only happen inside the blue area (points that satisfy the constraints) . # x = np.arange(2, 8, 0.01) # def y(x): # return (x-5)**2 # fig, ax = plt.subplots() # ax.plot(x, y(x)) # ax.set_xlabel(&quot;x&quot;) # ax.set_ylabel(&quot;y&quot;) # ax.fill_between(x, y(x), 9, color=&#39;blue&#39;, alpha=.1) # ax.plot([5], y(5), marker=&#39;o&#39;, color=&#39;black&#39;) # fig.savefig(&#39;my_icons/03_/two.png&#39;) . . . Now lets say we start at a point where $x=7$ and $y=4$ . # x = np.arange(2, 8, 0.01) # def y(x): # return (x-5)**2 # fig, ax = plt.subplots() # ax.plot(x, y(x)) # ax.set_xlabel(&quot;x&quot;) # ax.set_ylabel(&quot;y&quot;) # ax.fill_between(x, y(x), 9, color=&#39;blue&#39;, alpha=.1) # ax.plot([7], y(7), marker=&#39;o&#39;, color=&#39;red&#39;) # ax.plot([5], y(5), marker=&#39;o&#39;, color=&#39;black&#39;) # fig.savefig(&#39;my_icons/03_/three.png&#39;) . . . This red dot can go anywhere as long as it is in the blue area. Lets say this dot moves to $(6,6)$. This point is still in the blue area and satisfies the constraint we have set. This is called feasibility. . The process of gradient descent ONLY considers the solutions that are feasible (satisfies constraints) | . # x = np.arange(2, 8, 0.01) # def y(x): # return (x-5)**2 # fig, ax = plt.subplots() # ax.plot(x, y(x)) # ax.set_xlabel(&quot;x&quot;) # ax.set_ylabel(&quot;y&quot;) # ax.fill_between(x, y(x), 9, color=&#39;blue&#39;, alpha=.1) # ax.plot([7], y(7), marker=&#39;o&#39;, color=&#39;red&#39;) # ax.plot([6], 6, marker=&#39;o&#39;, color=&#39;green&#39;) # ax.plot([6], 2, marker=&#39;o&#39;, color=&#39;blue&#39;) # ax.plot([5], y(5), marker=&#39;o&#39;, color=&#39;black&#39;) # ax.arrow(7, y(7), 6-7+0.1, 2-y(7)+0.1, head_width=0.1, head_length=0.1); # fig.savefig(&#39;my_icons/03_/four.png&#39;) . . . While the point $(6,6)$ is feasible, does it go to the optimal value? The optimal value we want to find is the smallest point of $y=(x-5)^2$. . Y value when x=7 : 4 Y value when x=6 : 6 . The value of the function rather increased, and it means that this is not the right direction. . If the red dot goes to the direction of blue dot, the objective function decreases. This then would be the right direction we want to go. And the &#39;amount&#39; we want to move is the learning rate($ lambda$) which is a small step towards the direction. . The solution for each step is updated by $x^1 = x^0 + lambda d^0$, and the direction is where the optimal value gets closer to the global minimum. | . . 3. How is gradient calculated? . There are two aspects in calculating gradient. First is the &#39;direction&#39;, and the second is the &#39;distance&#39;. Lets say $d$ is for firection and $ alpha$ for step size (&#39;distance&#39;). Then this could be formulated as the following: $f(x^k + alpha d^k) &lt; f(x^k)$, which means the new value is smaller than the current function value. Using Taylor&#39;s expanson, it could be formulated as $f(x^k + alpha d^k) approx f(x^k) + alpha nabla f(x^k)^T d_k$ Since $f(x^k + alpha d^k) &lt; f(x^k)$, we want $ alpha nabla f(x^k)^T d_k$ to be smaller than zero. The steepest direction $d_k$ would be $- nabla f(x^k)$ since it is the opposite direction from $ nabla f(x^k)^T$ . Then, the new point is updated every step by $x^{k+1} leftarrow x_k - alpha nabla f(x_k)$ . This process will continue until $|| nabla f(x_k)|| leq epsilon$ where $ epsilon$ is a very small number. This is the point where the gradient is small enough or vanishes. This means that there is no more &#39;movement&#39;. . # x = np.arange(2, 8, 0.01) # def y(x): # return (x-5)**2 # fig, ax = plt.subplots() # ax.plot(x, y(x)) # ax.set_xlabel(&quot;x&quot;) # ax.set_ylabel(&quot;y&quot;) # ax.fill_between(x, y(x), 9, color=&#39;blue&#39;, alpha=.1) # ax.plot([7], y(7), marker=&#39;o&#39;, color=&#39;red&#39;) # ax.plot([5], y(5), marker=&#39;o&#39;, color=&#39;black&#39;) # def line1(x): # return 4*x - 24 # def line2(x): # return x*0 # ax.plot(np.arange(6, 8, 0.01), line1(np.arange(6, 8, 0.01)), color=&#39;red&#39;) # ax.plot(x, line2(x), color=&#39;black&#39;); # fig.savefig(&#39;my_icons/03_/five.png&#39;) . . . The red dot has a negative gradient, and it moves the point towards left. If the point reaches the black dot, the gradient is zero (vanishes), meaning there is no further direction for the point to move, indicating the end of the search. . . 4. Newton&#39;s method from scratch . One of the methods of gradient descent is Newton&#39;s method. . Let $x^k$ be the current state and consider a second order Taylor approximation of $f(x)$. $g(x) = f(x^k)+ nabla f(x^k)^T (x-x^k) + frac{1}{2}(x-x^k) nabla^2f(x^k)(x-x^k)$ . The next step would be to find the point that minimizes the function $g(x)$. That point would be where the first derivative equals to zero. $ nabla g(x) = nabla f(x^k)+ nabla^2f(x^k)(x-x^k) = 0$ . Solving this equation for $x$, this gives us $x = x^k - [ nabla^2f(x_k)]^{-1} nabla f(x^k)$ which will be the new $x^{k+1}$ . . To formulate this into the code, here is the psuedo code. . . Parameters . $k$ is the indicator for current step | $ epsilon$ is the minimum value for stopping the search | $ alpha$ is the step size. $ alpha$ changes via line search. This means that we want to find a step size that actually leads to minimizing the function value | $ rho$ and $c$ are scalars used to derive $ alpha$ during line search. They are between 0 and 1 | . Code walkthrough . Set the initial values for $ alpha$, $d$, $k$. Initial starting point $x^0$ will be provided via input | The search ends only if the first derivative of $f(x)$ is smaller than $ epsilon$. Here, it is actually the norm value of the vector. | Inside the search, first determine perform line search to find $ alpha$ If $ alpha$ is found, update $x^{k+1}$ as well as $d^{k+1}$ | . | Continue until vanishing gradient or when it is small enough | . Coding - from scratch . Lets solve the optimzal minimum value for the following function. $$y = 100(x_2-x_1^2)^2 + (1-x_1)^2$$ . def plot_contour_2d(X1, X2, y): fig, ax = plt.subplots(figsize=(12,7)) c_plot = ax.contour(X1, X2, y) ax.clabel(c_plot, inline=True, fontsize=10); ax.set_title(&#39;2D contour plot&#39;); def plot_contour_3d(X1, X2, y, save_dir): fig = plt.figure(figsize=(12,7)) ax = plt.axes(projection=&#39;3d&#39;) ax.contour3D(X1, X2, y, 50, cmap=&#39;binary&#39;) ax.set_xlabel(&quot;x1&quot;) ax.set_ylabel(&quot;x2&quot;) ax.set_zlabel(&quot;y&quot;) ax.set_title(&#39;3D contour plot&#39;); . . # X1 = np.arange(-50, 50, 0.1) # X2 = np.arange(-50, 20, 0.1) # X1, X2 = np.meshgrid(X1, X2) # y = 100 * (X2 - X1**2)**2 + (1-X1)**2 # plot_contour_3d(X1, X2, y, &#39;my_icons/03_/3D_contour.png&#39;) . . . # define x and y x1 = Symbol(&#39;x1&#39;) x2 = Symbol(&#39;x2&#39;) y = 100 * (x2 - x1**2)**2 + (1-x1)**2 . . print(&quot;Function Y&quot;) y . . Function Y . $ displaystyle left(1 - x_{1} right)^{2} + 100 left(- x_{1}^{2} + x_{2} right)^{2}$ # in order for the function to be calculated with x input, it has to be &#39;lambdafied&#39; # for more information, see the package sympy # it allows for place holders for the function, making it easy for us to read and understand def f(x1_value, x2_value, x1=x1, x2=x2, function=y): &quot;&quot;&quot; Input the values for x1 and x2, to the function &quot;&quot;&quot; working_function = lambdify((x1,x2), function, &#39;numpy&#39;) return working_function(x1_value, x2_value) def hessian_matrix(x1_value, x2_value, x1, x2, y): &quot;&quot;&quot; Calculates the hessian matrix for 2 variable function. &quot;&quot;&quot; matrix = np.zeros((2,2)) for i, der1 in enumerate([x1,x2]): for j, der2 in enumerate([x1,x2]): matrix[i,j] = f(x1_value, x2_value, x1, x2, y.diff(der1).diff(der2)) return matrix def first_order_matrix(x1_value, x2_value, x1, x2, y): matrix = np.zeros((2,1)) for i, der in enumerate([x1,x2]): matrix[i] = f(x1_value, x2_value, x1, x2, y.diff(der)) return matrix . . print(&quot;Partial derivative x1&quot;) y.diff(x1) . . Partial derivative x1 . $ displaystyle - 400 x_{1} left(- x_{1}^{2} + x_{2} right) + 2 x_{1} - 2$ print(&quot;Partial derivative x2&quot;) y.diff(x2) . . Partial derivative x2 . $ displaystyle - 200 x_{1}^{2} + 200 x_{2}$ print(&quot;Partial derivative x1, x2&quot;) y.diff(x1).diff(x2) . . Partial derivative x1, x2 . $ displaystyle - 400 x_{1}$ print(&quot;Partial derivative x2, x1&quot;) y.diff(x2).diff(x1) . . Partial derivative x2, x1 . $ displaystyle - 400 x_{1}$ Newton&#39;s method implemented . . END .",
            "url": "https://hooman34.github.io/Notes/optimization/gradient%20descent/2022/02/12/Mathematical_intution_of_gradient_descent.html",
            "relUrl": "/optimization/gradient%20descent/2022/02/12/Mathematical_intution_of_gradient_descent.html",
            "date": " • Feb 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Modeling extreme cases with peaks over threshold - 2",
            "content": "Packages used: extRemes, Rcpp, dplyr . Readings: An introduction to extreme value statistics, An introduction to statistical modeling of extreme values, 극단치이론을 이용한 VaR의 추정과 검증 : 국내 주식시장을 중심으로 . . Overview . &emsp;We&#39;ll dive right into the analysis of peaks over threshold, using sample data. Peaks over threshold method sets a threshold to sort out the data it wants to model. Also, it is a parametric method, meaning that we want to find parameters that would create a distribution that fits the data. &emsp;Below are the characteristics of peaks over threshold: . Follows a Generalized Pareto (GP) distribution | In mathematical terms, $G(y) = 1 - [1 + frac{ xi (y- mu)}{ sigma} ]^{ frac{- 1}{ xi}}$ $ mu$ is the threshold | $ sigma$ is the scale parameter | $ xi$ is the shape parameter | . | . options(warn=0) library(extRemes) library(Rcpp) library(ggplot2) library(dplyr) library(VGAM) library(repr) options(repr.plot.width=10, repr.plot.height=7) . . How the GP distribution looks like: . &emsp;Below is the plot where $ sigma$ changes while $ xi=0$ . x &lt;- seq(-4, 4, length=100) y1 &lt;- dgpd(x) y2 &lt;- dgpd(x, scale=0.5) y3 &lt;- dgpd(x, scale=0.3) y = append(y1, y2) y = append(y, y3) xs = append(x,x) xs = append(xs, x) labels = append(rep_len(&#39;scale=1&#39;, length(x)), rep_len(&#39;scale=0.5&#39;, length(x))) labels = append(labels, rep_len(&#39;scale=0.3&#39;, length(x))) df = data.frame(&#39;values&#39;=unlist(y)) df[&#39;x&#39;] = unlist(xs) df[&#39;labels&#39;] = unlist(labels) ggplot(df, aes(x=x, y=values, color=labels))+geom_line(size=1.5) . . &emsp;Below is the plot where $ xi$ changes while $ sigma=1$ &emsp;If $ xi&lt;0$, it means that this distribution has an upper bound. If $ xi=0$, it becomes $1-exp(- frac{y}{ tilde{ sigma}})$, which is an exponential distribution. It also means that extremes with lesser magnitudes have a higher probability of occurence than would be seen in distributions where $ xi&gt;0$. In other words, distributions with $ xi&gt;0$ would be a better fit for values at the very right end of the distribution. . x &lt;- seq(-4, 4, length=100) y1 &lt;- dgpd(x, shape=-0.5) y2 &lt;- dgpd(x, shape=0) y3 &lt;- dgpd(x, shape=0.5) y = append(y1, y2) y = append(y, y3) xs = append(x,x) xs = append(xs, x) labels = append(rep_len(&#39;shape=-0.5&#39;, length(x)), rep_len(&#39;shape=0&#39;, length(x))) labels = append(labels, rep_len(&#39;shape=0.5&#39;, length(x))) df = data.frame(&#39;values&#39;=unlist(y)) df[&#39;x&#39;] = unlist(xs) df[&#39;labels&#39;] = unlist(labels) ggplot(df, aes(x=x, y=values, color=labels))+geom_line(size=1.5) . . Starting off by creating sample data . &emsp;As explained before, we want to understand how frequent some extreme events will happen. &emsp;The data we will be working on is about the weather data from Fort Collins, Colorado, U.S.A. from 1900 to 1999, and focus on the precipitaion. Other datasets can be found in the extRemes package documentation page 4. &emsp;So in this case, it will be the return period of heavy rainfall. . &emsp;We&#39;ll go through the process of choosing the right threshold, fitting the distribution to the data, and assessing the result. . data(FCwx) df = FCwx head(df) . . A data.frame: 6 × 8 YearMnDyMxTMnTPrecSnowSnCv . &lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . 11900 | 1 | 1 | 39 | 10 | 0 | 0 | NA | . 21900 | 1 | 2 | 43 | 9 | 0 | 0 | NA | . 31900 | 1 | 3 | 55 | 23 | 0 | 0 | NA | . 41900 | 1 | 4 | 50 | 24 | 0 | 0 | NA | . 51900 | 1 | 5 | 50 | 16 | 0 | 0 | NA | . 61900 | 1 | 6 | 52 | 10 | 0 | 0 | NA | . &emsp;The histogram looks like below, and it seems that the precipitation goes up to 400. . hist(df$Prec) . . Setting the threshold . Now, to get the specific threshold, we will use the method proposed by Embrechts (2003); finding a threshold where the excess mean plot is linear. . Start from the mean of the pareto distribution $E(y)= frac{ sigma}{1- xi}$ where $ xi&lt;1$ | . | We hypothesized that the exceedent of $ mu_{0}$ follows a pareto distribution. Then the mean becomes $E(X- mu_{0}|X&gt; mu_{0})= frac{ sigma_{ mu_{0}}}{1- xi}$ | . | If the data points above threshold $ mu$ follows a pareto distribution, it also means that characteristics of pareto distribution should stand all thresholds above $ mu_0$. $E(X- mu_{0}|X&gt; mu_{0})= frac{ sigma_{ mu}}{1- xi}$ | = $ frac{ sigma_{ mu_{0}}+ xi mu}{1- xi}$ | . | Thus, we could say that $E(X- mu_{0}|X&gt; mu_{0})$ is a linear function of $ mu$. In other words, these estimates are expected to change linearly with $ mu$ when generalized distribution is appropriate. The appropriate threshold then would be the one where the mean residual life plot is approximately linear in $ mu$ | The mean residual life plot draws the mean excess for each threshold. | . | . quantile_1 = quantile(df$Prec, 0.994)[[1]] quantile_2 = quantile(df$Prec, 0.998)[[1]] mrlplot(df$Prec, xlim = c(0.9*quantile_1, 1.1*quantile_2)) . . &emsp;Usually, the area of interest should be set first. For now, lets say we are interested in cases where there are more than 100mm rainfall. &emsp;From the mean excess plot above, it seems the threshold that are close to 120 or 135 seems to have a flat mean excess. Those could be the potential threshold candidates. . &emsp;The next step would be to approximate the parameters for a sequence of parameters. As it was in the mean excess plot, we are trying to find a threshold that would have a stable distribution. Here, threshold around 105 or 120 seems okay. We also want to find the smallest threshold because that would be the first value that satisfies the following condition: &emsp;&emsp;$E(X- mu_{0}|X&gt; mu_{0})$ is a linear function of $ mu$. . threshrange.plot(df$Prec, r = c(0.9*quantile_1, 1.1*quantile_2), nint = 20) . . Lets try to fit a threshold to the Generalized pareto distribution. . df[is.na(df)] &lt;- 0 # input zero for missing precipiation values. The function does not allow missing values fitL &lt;- fevd(Prec, df, threshold = 105, type = &quot;GP&quot;, time.units = &quot;364/year&quot;, method=&#39;MLE&#39;) # time.units is set to 364/year since our data is a daily rainfall data plot(fitL) . . &emsp;The Q-Q plot on the left indicates that the current distribution is quite okay. However, are there better parameters? How do we know? . &emsp;As the threshold changes the distribution of the output, the VaR value driven by the distribution will also change. Thus, it is CRITICAL to choose a threshold where the result distribution is stable in order to have a reliable result. &emsp;The concept we could apply here would be to simply run all the possible experiments and choose the best threshold and parameters. Then, what does it mean to have the &#39;best threshold and parameters&#39;? . &emsp;The best parameters are the ones that best fit the data points. And we would use back-testing to compare the results of different thresholds and parameters. . Back-testing for model assessment . These are helper functions for this exercise. . calc_var = function(data_vector, threshL, fit_params, q){ # calculate the VaR for each quantile. rate = sum(data_vector &gt; threshL) / length(data_vector) scale = fitL[&#39;results&#39;][[1]][&#39;par&#39;][[1]][&#39;scale&#39;][[1]] shape = fitL[&#39;results&#39;][[1]][&#39;par&#39;][[1]][&#39;shape&#39;][[1]] var = threshL + (scale/shape) * ( ( (1/rate) * (1-q) )^(-1*shape) -1 ) return (var) } backtest = function(quantile_list, data_vector, threshL, fit_params, empirical){ # perform backtest # no need to calculate the VaR for empirical dataset # only need to fetch the value for each quantile if (empirical==TRUE){ observed = quantile(data_vector, probs = quantile_list) q_df = as.data.frame(observed) q_df[&#39;num_data_used&#39;] = rep.int(length(data_vector), length(quantile_list)) q_df[&#39;quantiles&#39;] = unlist(quantile_list) rownames(q_df) = NULL } else{ # for already fitted distribution, calculate the value that corresponds for each quantile observed = list() for (q in quantile_list){ VaR = calc_var(data_vector, threshL, fit_params, q) observed = append(observed, VaR) } q_df = as.data.frame(unlist(observed)) colnames(q_df) = c(&#39;observed&#39;) q_df[&#39;num_data_used&#39;] = rep.int(sum(data_vector &gt; threshL), length(quantile_list)) q_df[&#39;quantiles&#39;] = unlist(quantile_list) rownames(q_df) = NULL } num_over_thresh_list = list() for (th in q_df$observed){ num_over_thresh = sum(data_vector&gt;th) num_over_thresh_list = append(num_over_thresh_list, num_over_thresh) } q_df[&#39;num_over_thresh&#39;] = unlist(num_over_thresh_list) return (q_df) } . . The basic concept of back-testing here is like this: . If the distribution correctly represents the data, the percentage of the rainfall exceeding VaR should be equal to the percentage of the rainfall exceeding 1-q(quantile) of the actual data. Lets go through an example. | . . Quantile 0.999. . Lets say the value for 0.999 quantile of the observational data is $a$. There are 7 observations that goes over $a$. | For GPD distribution where the threshold is 1.7659, lets say the value for 0.999 quantile is $b$. There are 6 observations that goes over $b$ | The GP distribution that is closest to the (1-q) observations for each quantile would be the most appropriate one. | . In order to come up with the VaR value for each quantile, we could use the GP distribution and the estimated parameters . $ hat F(x) = 1- rate*[1+ frac{ hat xi}{ hat sigma}(x- mu)]^{- frac{1}{ hat xi}}$ . $ mu$ is the threshold | $ sigma$ is the scale parameter | $ xi$ is the shape parameter | $rate= frac{num ;data ;points ;over ;threshold}{num ;data ;points}$ | . Using the calculation above, the VaR would be: $VaR_{q} = mu + frac{ hat sigma}{ hat xi}[ { { frac{1}{rate}(1-q)} }^{- xi} -1]$ where $q&gt;F( mu)$ . The experiment would be the following: . Test for different thresholds. Since 105 and 120 are potential candidates, test with different threshold around those values | . | Compare the number of datat that exceeds different quantiles. Compare between the results from empirical dataset and the distribution | . | . threshold_list = c(100, 105, 107, 110, 115, 120, 125, 130) quantile_list = c(0.99900, 0.9995, 0.9996, 0.9997, 0.9998, 0.99983, 0.99985, 0.99987, 0.99990, 0.99993, 0.99995, 0.99997, 0.99999) . threshold_list = c(100, 105, 107, 110, 115, 120, 125, 130) quantile_list = c(0.99900, 0.9995, 0.9996, 0.9997, 0.9998, 0.99983, 0.99985, 0.99987, 0.99990, 0.99993, 0.99995, 0.99997, 0.99999) backtest_df = backtest(quantile_list, df$Prec, None, fit_params, empirical = TRUE) backtest_df[&#39;threshold&#39;] = rep_len(&quot;Empirical&quot;, length(nrow(backtest_df))) print(&quot;Back test results for empirical dataset.&quot;) print(backtest_df) . . [1] &#34;Back test results for empirical dataset.&#34; observed num_data_used quantiles num_over_thresh threshold 1 199.0000 36524 0.99900 36 Empirical 2 239.7385 36524 0.99950 19 Empirical 3 262.2988 36524 0.99960 15 Empirical 4 297.0431 36524 0.99970 11 Empirical 5 304.7816 36524 0.99980 8 Empirical 6 317.8663 36524 0.99983 7 Empirical 7 335.0818 36524 0.99985 6 Empirical 8 349.5121 36524 0.99987 5 Empirical 9 354.0000 36524 0.99990 3 Empirical 10 389.4712 36524 0.99993 3 Empirical 11 435.5646 36524 0.99995 2 Empirical 12 442.1388 36524 0.99997 2 Empirical 13 455.6954 36524 0.99999 1 Empirical . There are 36 data points that goes over 199mm. 19 that goes over 239.7 ... | The target is to find backtesting results that would have similar results as this one. | The number of data points that goes over a certain value indicates the return period. Having similar back-testing result would mean that the distribution correctly reflects the return period. | . Experiment1: Threshold 100 . threshL = 100 fitL &lt;- fevd(Prec, df, threshold = threshL, type = &quot;GP&quot;, time.units = &quot;364/year&quot;, method = &quot;MLE&quot;) fit_params &lt;- fitL backtest_df = backtest(quantile_list, df$Prec, threshL, fit_params, empirical=FALSE) backtest_df[&#39;threshold&#39;] = rep.int(threshL, length(nrow(backtest_df))) print(paste(&quot;Back test results for threshold:&quot;, threshL)) print(backtest_df) . . [1] &#34;Back test results for threshold: 100&#34; observed num_data_used quantiles num_over_thresh threshold 1 201.1260 213 0.99900 35 100 2 245.9984 213 0.99950 17 100 3 261.1154 213 0.99960 15 100 4 281.1067 213 0.99970 13 100 5 310.2728 213 0.99980 7 100 6 322.2977 213 0.99983 6 100 7 331.6921 213 0.99985 6 100 8 342.5772 213 0.99987 6 100 9 362.9414 213 0.99990 3 100 10 391.4928 213 0.99993 3 100 11 419.3722 213 0.99995 3 100 12 463.5238 213 0.99997 0 100 13 566.4377 213 0.99999 0 100 . Experiment1: Threshold 105 . threshL = 105 fitL &lt;- fevd(Prec, df, threshold = threshL, type = &quot;GP&quot;, time.units = &quot;364/year&quot;, method = &quot;MLE&quot;) fit_params &lt;- fitL backtest_df = backtest(quantile_list, df$Prec, threshL, fit_params, empirical=FALSE) backtest_df[&#39;threshold&#39;] = rep.int(threshL, length(nrow(backtest_df))) print(paste(&quot;Back test results for threshold:&quot;, threshL)) print(backtest_df) . . [1] &#34;Back test results for threshold: 105&#34; observed num_data_used quantiles num_over_thresh threshold 1 203.3588 187 0.99900 34 105 2 247.7954 187 0.99950 17 105 3 262.4579 187 0.99960 15 105 4 281.6233 187 0.99970 13 105 5 309.1453 187 0.99980 7 105 6 320.3468 187 0.99983 7 105 7 329.0407 187 0.99985 6 105 8 339.0527 187 0.99987 6 105 9 357.6109 187 0.99990 3 105 10 383.2649 187 0.99993 3 105 11 407.9224 187 0.99995 3 105 12 446.2225 187 0.99997 1 105 13 532.2563 187 0.99999 0 105 . Experiment1: Threshold 110 . threshL = 110 fitL &lt;- fevd(Prec, df, threshold = threshL, type = &quot;GP&quot;, time.units = &quot;364/year&quot;, method = &quot;MLE&quot;) fit_params &lt;- fitL backtest_df = backtest(quantile_list, df$Prec, threshL, fit_params, empirical=FALSE) backtest_df[&#39;threshold&#39;] = rep.int(threshL, length(nrow(backtest_df))) print(paste(&quot;Back test results for threshold:&quot;, threshL)) print(backtest_df) . . [1] &#34;Back test results for threshold: 110&#34; observed num_data_used quantiles num_over_thresh threshold 1 202.4413 174 0.99900 35 110 2 246.9875 174 0.99950 17 110 3 261.8145 174 0.99960 15 110 4 281.2898 174 0.99970 13 110 5 309.4425 174 0.99980 7 110 6 320.9628 174 0.99983 7 110 7 329.9287 174 0.99985 6 110 8 340.2804 174 0.99987 6 110 9 359.5426 174 0.99990 3 110 10 386.3278 174 0.99993 3 110 11 412.2432 174 0.99995 3 110 12 452.8246 174 0.99997 1 110 13 545.4017 174 0.99999 0 110 . Experiment1: Threshold 120 . threshL = 120 fitL &lt;- fevd(Prec, df, threshold = threshL, type = &quot;GP&quot;, time.units = &quot;364/year&quot;, method = &quot;MLE&quot;) fit_params &lt;- fitL backtest_df = backtest(quantile_list, df$Prec, threshL, fit_params, empirical=FALSE) backtest_df[&#39;threshold&#39;] = rep.int(threshL, length(nrow(backtest_df))) print(paste(&quot;Back test results for threshold:&quot;, threshL)) print(backtest_df) . . [1] &#34;Back test results for threshold: 120&#34; observed num_data_used quantiles num_over_thresh threshold 1 206.0287 138 0.99900 32 120 2 250.9731 138 0.99950 16 120 3 265.4544 138 0.99960 15 120 4 284.1330 138 0.99970 13 120 5 310.4762 138 0.99980 7 120 6 321.0407 138 0.99983 6 120 7 329.1792 138 0.99985 6 120 8 338.4863 138 0.99987 6 120 9 355.5567 138 0.99990 3 120 10 378.7767 138 0.99993 3 120 11 400.6958 138 0.99995 3 120 12 433.9995 138 0.99997 3 120 13 505.7326 138 0.99999 0 120 . Overall, here are the findings: . The backtest results for threshold 105 and 110 are most similar to the empirical results | Threshold 105 represents the empirical dataset better, than threshold 100 Could see it captures the value at 0.99997 while the distribution with threshold 100 cannot | . | Increasing the threshold to 110 has similar result meaning the &#39;look&#39; of the distribution is stable. Thus 105 is an appropriate choice | Increasing the threshold to 120 deviates the back-testing results from the empirical observations | . Final look of the model . threshL = 105 fitL &lt;- fevd(Prec, df, threshold = threshL, type = &quot;GP&quot;, time.units = &quot;364/year&quot;, method = &quot;MLE&quot;) summary(fitL) . . fevd(x = Prec, data = df, threshold = threshL, type = &#34;GP&#34;, method = &#34;MLE&#34;, time.units = &#34;364/year&#34;) [1] &#34;Estimation Method used: MLE&#34; Negative Log-Likelihood Value: 955.3269 Estimated parameters: scale shape 57.61372105 0.05392585 Standard Error Estimates: scale shape 6.23291161 0.07997853 Estimated parameter covariance matrix. scale shape scale 38.8491872 -0.350636171 shape -0.3506362 0.006396566 AIC = 1914.654 BIC = 1921.116 . plot(fitL) . . Utilizing the model . With the distribution we derived, we could calculate the return period of extreme events. The return period would be the inverse of $[1 + frac{ xi (y- mu)}{ sigma} ]^{ frac{- 1}{ xi}}*rate*ndays$, where $rate= frac{num ;data ;points ;over ;threshold}{num ;data ;points}$ . return.level(fitL, return.period = 2:10) . . fevd(x = Prec, data = df, threshold = threshL, type = &#34;GP&#34;, method = &#34;MLE&#34;, time.units = &#34;364/year&#34;) get(paste(&#34;return.level.fevd.&#34;, newcl, sep = &#34;&#34;))(x = x, return.period = return.period) GP model fitted to Prec df Data are assumed to be stationary [1] &#34;Covariate data = df&#34; [1] &#34;Return Levels for period units in years&#34; 2-year level 3-year level 4-year level 5-year level 6-year level 183.5553 208.9094 227.2377 241.6512 253.5574 7-year level 8-year level 9-year level 10-year level 263.7157 272.5837 280.4590 287.5463 . And for very extreme cases such as 200 year return period, the result is like below. 506mm of rain will fall in every 200 years. . return.level(fitL, return.period = 200) . . fevd(x = Prec, data = df, threshold = threshL, type = &#34;GP&#34;, method = &#34;MLE&#34;, time.units = &#34;364/year&#34;) get(paste(&#34;return.level.fevd.&#34;, newcl, sep = &#34;&#34;))(x = x, return.period = return.period) GP model fitted to Prec df Data are assumed to be stationary [1] &#34;Covariate data = df&#34; [1] &#34;Return Levels for period units in years&#34; 200-year level 506.8702 . . END .",
            "url": "https://hooman34.github.io/Notes/extreme%20value%20statistics/peaks%20over%20threshold/statistics/2022/01/30/Modeling_extreme_cases_with_peaks_over_threshold_2.html",
            "relUrl": "/extreme%20value%20statistics/peaks%20over%20threshold/statistics/2022/01/30/Modeling_extreme_cases_with_peaks_over_threshold_2.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Modeling extreme cases with peaks over threshold - 1",
            "content": "Relevant materials: An introduction to extreme value statistics An introduction to statistical modeling of extreme values 극단치이론을 이용한 VaR의 추정과 검증 : 국내 주식시장을 중심으로 FRM: Extreme Value Theory (EVT) - Intro FRM: Three approaches to value at risk (VaR) . . What we want to model . &emsp;Lets say you want to model a rainfall that comes every 250 years. Enough rainfall would lead to floods, and would then lead to damages to the shop owners or vehicle owners. &emsp;There are other extreme events besides rainfall. Those could be a stock market crash or any other events that does not happen a lot, but when it does, would have a huge impact. . How we can model extreme cases . &emsp;Overall, when assessing the value of something, we use a Value At Risk (VAR) methods. There are parametric and non-parametric methods under VAR, and this article focuses on parametric methods. &emsp;&quot;Parametric&quot; means it uses parameters. In other words, we want to come out with a parameter so that the model based on those parameters would appropriately represent the data. . For modeling extreme cases, we could choose between two methods. . Block maxima | Peaks over threshold | &emsp;Both methods aim to fit a distribution to extreme cases, which looks like this: . . &emsp;A normal distribution won&#39;t be a good fit since it doesn&#39;t take much consideration on the far right or left data points. Thus we cut the tail of the distribution. . Block maxima . &emsp;The block maxima method chooses the maximum value for each time span e.g. year. The data points extracted like this tends to follow a GEV (Generalized Extreme Value) distribution. However, the extimated Value At Risk calculated based on this method tends to be unstable according to the specified time span. &emsp;We will focus on a more stable method; peaks over threshold. . . Peaks over threshold . &emsp;As the name mentiones, this methods sets a threshold and tries to fit a distribution for data points that goes over that threshold. . . &emsp;Then it would mean that choosing the right threshold is critical in the process of this method. &emsp;Here are the methods introduced for choosing the appropriate threshold. . Paper Proposed way of setting the threshold . Davison and Smith (1990) / Nefci (2000) | Set it as (standard dev * 1.65) | . Danielsson and de Vires (1997) | Set threshold based on MSE | . Cole S. (2001) | Use mean excess graph and Hill graph | . Massahiro F. and Yasufumi S. (2002) | Value where it divides 4~6% of the total data | . Christoffersen (2003) | Value where it divides 5% of the total data | . Embrechts (2003) | Where the excess mean plot is linear | . &emsp;The hard part of modeling extreme values is that there are only a few data points. This makes it hard to determine which part of the tail we should &#39;cut&#39;. Naturally, it becomes important to find parameters for a distribution that can have stable results with minimal loss. . In the next post, we&#39;ll look into the mathematical details of peaks overthreshold. . .",
            "url": "https://hooman34.github.io/Notes/extreme%20value%20statistics/peaks%20over%20threshold/statistics/2022/01/28/Modeling_extreme_cases_with_peaks_over_threshold.html",
            "relUrl": "/extreme%20value%20statistics/peaks%20over%20threshold/statistics/2022/01/28/Modeling_extreme_cases_with_peaks_over_threshold.html",
            "date": " • Jan 28, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Portfolio optimization using cvxpy",
            "content": "1. Objective . Would like to know how much investment should go into each stocks, in order to optimize the portfolio. Here, optimization means . expected return exceeds minimum threshold | minimize the risk of the portfolio return | . 2. Components of optimization . 2.1 Decision variable . Matrix X refers to a portion of each individual stock $X = begin{bmatrix} x_1 x_2 x_3 end{bmatrix}$ Here, the constraint is: $X geq0$ . 2.2 Constraints . Budget constraint $e^Tx = 1.0$ where $e=[1,1,1]$ | In other words, the investment portion of each stock should sum up to 1 | . | Expected return constraint We want our expected return of our portfolio to be higher than a certain threshold | $ mathbb E begin{bmatrix} sum_{i=1}^{3} tilde{r}_i x_i end{bmatrix} = sum_{i=1}^{3} mathbb E begin{bmatrix} tilde{r}_i end{bmatrix}x_i = sum_{i=1}^{3} bar{r}_i x_i$ | This could be the same this as $ bar{r}^T X$ which means the sum of multiplication of average return and investment portion | Thus, $ sum_{i=1}^{3} bar{r}_i x_i geq r_{min}$ | $r_{min}$ would be set based on our judgement | . | . 2.3 Objective function . This is the criteria for choosing the best set of decision. . It is to minimize the variance of the portfolio returns $x^TQx$ where $Q$ is a covariance matrix | . | . 2.4 Assumption . It is assumed that the monthly stock returns have a stationary proability distribution. This means that it has a fixed distribution, and this also means that the projections done based on historical data is valid. . 3. Analysis . import pandas as pd import numpy as np from cvxpy import * import pandas_datareader as pdr . 3.1 Read ticker data . It is easy to read ticker data using pandas-datareader . tickers = [&#39;MSFT&#39;, &#39;V&#39;, &#39;WMT&#39;] start_date = &#39;2019-01-02&#39; end_date = &#39;2021-12-31&#39; stock_price = pdr.DataReader(tickers, &#39;yahoo&#39;, start_date, end_date) stock_price.head(3) . Attributes Adj Close Close High Low Open Volume . Symbols MSFT V WMT MSFT V WMT MSFT V WMT MSFT V WMT MSFT V WMT MSFT V WMT . Date . 2019-01-02 97.782417 | 130.463150 | 88.576424 | 101.120003 | 132.919998 | 93.339996 | 101.750000 | 133.740005 | 93.650002 | 98.940002 | 129.600006 | 91.639999 | 99.550003 | 130.000000 | 91.639999 | 35329300.0 | 8788000.0 | 8152700.0 | . 2019-01-03 94.185211 | 125.761711 | 88.120903 | 97.400002 | 128.130005 | 92.860001 | 100.190002 | 131.279999 | 94.709999 | 97.199997 | 127.879997 | 92.699997 | 100.099998 | 131.210007 | 93.209999 | 42579100.0 | 9428300.0 | 8277300.0 | . 2019-01-04 98.565704 | 131.179657 | 88.671318 | 101.930000 | 133.649994 | 93.440002 | 102.510002 | 134.589996 | 93.660004 | 98.930000 | 130.130005 | 92.690002 | 99.720001 | 130.440002 | 93.209999 | 44060600.0 | 11065800.0 | 8029100.0 | . 3.2 Derive monthly return . We&#39;ll just take the close price for each ticker. The monthly return will be used for simplicity. . stock_price = stock_price[[( &#39;Close&#39;, &#39;MSFT&#39;), ( &#39;Close&#39;, &#39;V&#39;), ( &#39;Close&#39;, &#39;WMT&#39;)]] stock_price.columns = [&#39;MSFT&#39;, &#39;V&#39;, &#39;WMT&#39;] stock_price.reset_index(inplace=True) stock_price.head(3) . Date MSFT V WMT . 0 2019-01-02 | 101.120003 | 132.919998 | 93.339996 | . 1 2019-01-03 | 97.400002 | 128.130005 | 92.860001 | . 2 2019-01-04 | 101.930000 | 133.649994 | 93.440002 | . stock_price[&#39;year&#39;] = stock_price.Date.dt.year stock_price[&#39;month&#39;] = stock_price.Date.dt.month stock_price = stock_price.groupby([&#39;year&#39;, &#39;month&#39;]).mean().reset_index(drop=True) stock_price.head(3) . MSFT V WMT . 0 104.135238 | 136.442381 | 95.809047 | . 1 107.927894 | 143.581579 | 97.775263 | . 2 115.133810 | 152.260954 | 98.275714 | . $$stockReturn = frac{price_t-price_{t-1}}{price_{t-1}}$$ . stock_price_shift = stock_price.shift().rename(columns={&quot;MSFT&quot;:&quot;MSFT_sft&quot;, &quot;V&quot;:&quot;V_sft&quot;, &quot;WMT&quot;:&quot;WMT_sft&quot;}) stock_price = pd.concat([stock_price, stock_price_shift[[&#39;MSFT_sft&#39;, &#39;V_sft&#39;, &#39;WMT_sft&#39;]]], axis=1) stock_price.head(3) . MSFT V WMT MSFT_sft V_sft WMT_sft . 0 104.135238 | 136.442381 | 95.809047 | NaN | NaN | NaN | . 1 107.927894 | 143.581579 | 97.775263 | 104.135238 | 136.442381 | 95.809047 | . 2 115.133810 | 152.260954 | 98.275714 | 107.927894 | 143.581579 | 97.775263 | . monthly_returns = pd.DataFrame() for ticker in [&#39;MSFT&#39;, &#39;V&#39;, &#39;WMT&#39;]: returns = (stock_price[ticker] - stock_price[ticker+&#39;_sft&#39;]) / stock_price[ticker+&#39;_sft&#39;] monthly_returns = pd.concat([monthly_returns, pd.DataFrame(returns)], axis=1) # rename the column names monthly_returns.columns = [&#39;MSFT&#39;, &#39;V&#39;, &#39;WMT&#39;] # drop first row since it is not used when calculating stock returns monthly_returns.drop(0, axis=0, inplace=True) monthly_returns.head(3) . MSFT V WMT . 1 0.036420 | 0.052324 | 0.020522 | . 2 0.066766 | 0.060449 | 0.005118 | . 3 0.066540 | 0.050027 | 0.026698 | . 4. Specify the components for optimization . To reiterate, the objective of this analysis is to build a portfolio that has minimum risk. And while there are many ways to represent risk, we will use the variance of the return. . print(&quot;Average monthly return over the two years.&quot;) monthly_returns.mean().reset_index() . Average monthly return over the two years. . index 0 . 0 MSFT | 0.034952 | . 1 V | 0.013684 | . 2 WMT | 0.011622 | . print(&quot;Standard deviation of each stock over two years.&quot;) monthly_returns.std().reset_index() . Standard deviation of each stock over two years. . index 0 . 0 MSFT | 0.049329 | . 1 V | 0.048437 | . 2 WMT | 0.035628 | . Now, in order to utilize cvxpy to optimize the current problem, we would have to specify the components we have identified earlier. . Objective . minimize $x^TQx$ where $Q$ is a covariance matrix (risk) | . Constraints . $X&gt;0$ where X is a portion of budget that goes into each stock | $ sum X=1$ Investment portion of each stock should sum up to 1 | $ sum_{i=1}^{3} bar{r}_i x_i geq r_{min}$ | . For $r_{min}$, lets set it to 2% since Microsoft has the highest monthly average return, and it is 3.5%. Realistically, a portfolio which is a combination of other stocks would have lower average return. . symbols = monthly_returns.columns.to_list() n = len(symbols) x = Variable(n) min_return = 0.02 r = monthly_returns.mean().values # potfolio_return = multiply(r.T, x) potfolio_return = r.T @ x # objective is to minimize risk covariance_matrix = np.asmatrix(np.cov(monthly_returns.values.T)) risk = quad_form(x, covariance_matrix) # set the optimization problem prob = Problem(Minimize(risk), [sum(x) == 1, potfolio_return &gt;= min_return, x &gt;= 0]) . 5. Solve optimization using cvxpy . prob.solve() . 0.0009641333660832668 . print(&quot;Here is the portion of each stock in order to minimize risk while the return is bigger than 2%. n&quot;) for i in range(len(symbols)): print(&quot;{}: {:.2f}%&quot;.format(symbols[i], x.value[i]*100)) print() print(&quot;The volatility of this portfolio is {:.5f}%&quot;.format(risk.value**0.5*100)) print(&quot;The return of this portfolio is {}%&quot;.format(round(potfolio_return.value*100, 0))) . Here is the portion of each stock in order to minimize risk while the return is bigger than 2%. MSFT: 34.33% V: 17.87% WMT: 47.79% The volatility of this portfolio is 3.10505% The return of this portfolio is 2.0% . . You can check out for the official example of csvpy for portfolio optimization here .",
            "url": "https://hooman34.github.io/Notes/optimization/cvxpy/2022/01/13/Portfolio_optimization_using_cvxpy.html",
            "relUrl": "/optimization/cvxpy/2022/01/13/Portfolio_optimization_using_cvxpy.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hooman34.github.io/Notes/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hooman34.github.io/Notes/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hooman34.github.io/Notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hooman34.github.io/Notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}