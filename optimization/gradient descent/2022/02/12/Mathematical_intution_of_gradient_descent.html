<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Mathematical intuition of gradient descent | Notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Mathematical intuition of gradient descent" />
<meta name="author" content="Gieun Kwak" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understanding and coding gradient descent from scratch" />
<meta property="og:description" content="Understanding and coding gradient descent from scratch" />
<link rel="canonical" href="https://hooman34.github.io/Notes/optimization/gradient%20descent/2022/02/12/Mathematical_intution_of_gradient_descent.html" />
<meta property="og:url" content="https://hooman34.github.io/Notes/optimization/gradient%20descent/2022/02/12/Mathematical_intution_of_gradient_descent.html" />
<meta property="og:site_name" content="Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-12T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Mathematical intuition of gradient descent" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Gieun Kwak"},"dateModified":"2022-02-12T00:00:00-06:00","datePublished":"2022-02-12T00:00:00-06:00","description":"Understanding and coding gradient descent from scratch","headline":"Mathematical intuition of gradient descent","mainEntityOfPage":{"@type":"WebPage","@id":"https://hooman34.github.io/Notes/optimization/gradient%20descent/2022/02/12/Mathematical_intution_of_gradient_descent.html"},"url":"https://hooman34.github.io/Notes/optimization/gradient%20descent/2022/02/12/Mathematical_intution_of_gradient_descent.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://hooman34.github.io/Notes/feed.xml" title="Notes" /><link rel="shortcut icon" type="image/x-icon" href="/Notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Notes/">Notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Notes/about/">About Me</a><a class="page-link" href="/Notes/search/">Search</a><a class="page-link" href="/Notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Mathematical intuition of gradient descent</h1><p class="page-description">Understanding and coding gradient descent from scratch</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-12T00:00:00-06:00" itemprop="datePublished">
        Feb 12, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Gieun Kwak</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Notes/categories/#optimization">optimization</a>
        &nbsp;
      
        <a class="category-tags-link" href="/Notes/categories/#gradient descent">gradient descent</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/hooman34/Notes/tree/master/_notebooks/2022-02-12-Mathematical_intution_of_gradient_descent.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Notes/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/hooman34/Notes/master?filepath=_notebooks%2F2022-02-12-Mathematical_intution_of_gradient_descent.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Notes/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/hooman34/Notes/blob/master/_notebooks/2022-02-12-Mathematical_intution_of_gradient_descent.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Notes/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fhooman34%2FNotes%2Fblob%2Fmaster%2F_notebooks%2F2022-02-12-Mathematical_intution_of_gradient_descent.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/Notes/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-How-gradient-descent-looks-like">1. How gradient descent looks like </a></li>
<li class="toc-entry toc-h3"><a href="#2.-How-gradient-descent-works:-simple-maths">2. How gradient descent works: simple maths </a></li>
<li class="toc-entry toc-h3"><a href="#3.-How-is-gradient-calculated?">3. How is gradient calculated? </a></li>
<li class="toc-entry toc-h3"><a href="#4.-Newton's-method-from-scratch">4. Newton&#39;s method from scratch </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Parameters">Parameters </a></li>
<li class="toc-entry toc-h4"><a href="#Code-walkthrough">Code walkthrough </a></li>
<li class="toc-entry toc-h4"><a href="#Coding---from-scratch">Coding - from scratch </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-12-Mathematical_intution_of_gradient_descent.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sympy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="c1"># from IPython.display import set_matplotlib_formats</span>
<span class="c1"># import matplotlib_inline.backend_inline</span>
<span class="c1"># matplotlib_inline.backend_inline.set_matplotlib_formats('png')</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-How-gradient-descent-looks-like">
<a class="anchor" href="#1.-How-gradient-descent-looks-like" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. How gradient descent looks like<a class="anchor-link" href="#1.-How-gradient-descent-looks-like"> </a>
</h3>
<p>Gradient descent is commonly used in ML and AI. Looking at the image below, we encounter from lectures that the gradient descent is a 'process of finding the most lowest point of the function'. While this is a very concise and accurate explanation of gradient descent, understanding it in a mathematical way would enable a deeper understanding.</p>
<p><img src="/Notes/images/copied_from_nb/my_icons/03_/gradient-descent-example.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br>
<br>
<br>
<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-How-gradient-descent-works:-simple-maths">
<a class="anchor" href="#2.-How-gradient-descent-works:-simple-maths" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. How gradient descent works: simple maths<a class="anchor-link" href="#2.-How-gradient-descent-works:-simple-maths"> </a>
</h3>
<p>The gradient descent image above goes through a line until it reaches the 'bottom' of the function. This 'bottom' of the function is called 'minimum'. We want to reach the minimum because it will give us the best values for our problem. One of the common use of gradient is when we estimate the parameters of a ML or AI model. We want to find the paramaters that has the smalles loss value (closes to the actual observation).</p>
<p>Lets consider a function $y = (x-5)^2$. We want to find the minimum value of this function by following a sequence of steps.</p>
<ol>
<li>Start from a certain point</li>
<li>Move to a new point that gives a better objective value (here, smaller values)</li>
<li>Repeat step 2</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># x = np.arange(2, 8, 0.01)</span>
<span class="c1"># def y(x):</span>
<span class="c1">#     return (x-5)**2</span>

<span class="c1"># fig, ax = plt.subplots()</span>
<span class="c1"># ax.plot(x, y(x))</span>
<span class="c1"># ax.set_xlabel("x")</span>
<span class="c1"># ax.set_ylabel("y")</span>
<span class="c1"># ax.plot([5], y(5), marker='o', color='black')</span>

<span class="c1"># fig.savefig('my_icons/03_/one.png');</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Notes/images/copied_from_nb/my_icons/03_/one.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One thing most explanations exclude is the constraint. Actually, for every optimization problem, there is an objective function and a constraint. The objective function is $y = (x-5)^2$. And the constraint is hidden, which is $x \in R$, $y \in R$ ($R$ means real numbers).</p>
<p>The optimization problem changes along the constraints, but for now lets say the constraint is $y \geq (x-5)^2$. This means the set of potential solutions for this problem is all the points above the function line.</p>
<p>The process of gradient descent will only happen inside the blue area (points that satisfy the constraints)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># x = np.arange(2, 8, 0.01)</span>
<span class="c1"># def y(x):</span>
<span class="c1">#     return (x-5)**2</span>

<span class="c1"># fig, ax = plt.subplots()</span>
<span class="c1"># ax.plot(x, y(x))</span>
<span class="c1"># ax.set_xlabel("x")</span>
<span class="c1"># ax.set_ylabel("y")</span>

<span class="c1"># ax.fill_between(x, y(x), 9, color='blue', alpha=.1)</span>
<span class="c1"># ax.plot([5], y(5), marker='o', color='black')</span>

<span class="c1"># fig.savefig('my_icons/03_/two.png')</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Notes/images/copied_from_nb/my_icons/03_/two.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now lets say we start at a point where $x=7$ and $y=4$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># x = np.arange(2, 8, 0.01)</span>
<span class="c1"># def y(x):</span>
<span class="c1">#     return (x-5)**2</span>

<span class="c1"># fig, ax = plt.subplots()</span>
<span class="c1"># ax.plot(x, y(x))</span>
<span class="c1"># ax.set_xlabel("x")</span>
<span class="c1"># ax.set_ylabel("y")</span>
<span class="c1"># ax.fill_between(x, y(x), 9, color='blue', alpha=.1)</span>

<span class="c1"># ax.plot([7], y(7), marker='o', color='red')</span>
<span class="c1"># ax.plot([5], y(5), marker='o', color='black')</span>

<span class="c1"># fig.savefig('my_icons/03_/three.png')</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Notes/images/copied_from_nb/my_icons/03_/three.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This red dot can go anywhere as long as it is in the blue area. Lets say this dot moves to $(6,6)$. This point is still in the blue area and satisfies the constraint we have set. This is called feasibility.</p>
<ul>
<li><strong>The process of gradient descent ONLY considers the solutions that are feasible (satisfies constraints)</strong></li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># x = np.arange(2, 8, 0.01)</span>
<span class="c1"># def y(x):</span>
<span class="c1">#     return (x-5)**2</span>

<span class="c1"># fig, ax = plt.subplots()</span>
<span class="c1"># ax.plot(x, y(x))</span>
<span class="c1"># ax.set_xlabel("x")</span>
<span class="c1"># ax.set_ylabel("y")</span>
<span class="c1"># ax.fill_between(x, y(x), 9, color='blue', alpha=.1)</span>

<span class="c1"># ax.plot([7], y(7), marker='o', color='red')</span>
<span class="c1"># ax.plot([6], 6, marker='o', color='green')</span>
<span class="c1"># ax.plot([6], 2, marker='o', color='blue')</span>
<span class="c1"># ax.plot([5], y(5), marker='o', color='black')</span>

<span class="c1"># ax.arrow(7, y(7), 6-7+0.1, 2-y(7)+0.1, head_width=0.1, head_length=0.1);</span>

<span class="c1"># fig.savefig('my_icons/03_/four.png')</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Notes/images/copied_from_nb/my_icons/03_/four.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While the point $(6,6)$ is feasible, does it go to the optimal value? The optimal value we want to find is the smallest point of $y=(x-5)^2$.</p>
<p>Y value when x=7 : 4 <br>
Y value when x=6 : 6</p>
<p>The value of the function rather increased, and it means that this is not the right direction.</p>
<p>If the red dot goes to the direction of blue dot, the objective function decreases. This then would be the right direction we want to go. And the 'amount' we want to move is the learning rate($\lambda$) which is a small step towards the direction.</p>
<ul>
<li><strong>The solution for each step is updated by $x^1 = x^0 + \lambda d^0$, and the direction is where the optimal value gets closer to the global minimum.</strong></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br>
<br>
<br>
<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-How-is-gradient-calculated?">
<a class="anchor" href="#3.-How-is-gradient-calculated?" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. How is gradient calculated?<a class="anchor-link" href="#3.-How-is-gradient-calculated?"> </a>
</h3>
<p>There are two aspects in calculating gradient. First is the 'direction', and the second is the 'distance'. <br> Lets say $d$ is for firection and $\alpha$ for step size ('distance'). Then this could be formulated as the following: $f(x^k + \alpha d^k) &lt; f(x^k)$, which means the new value is smaller than the current function value. <br>
Using Taylor's expanson, it could be formulated as $f(x^k + \alpha d^k) \approx f(x^k) + \alpha\nabla f(x^k)^T d_k$ <br>
Since $f(x^k + \alpha d^k) &lt; f(x^k)$, we want $\alpha\nabla f(x^k)^T d_k$ to be smaller than zero. The steepest direction $d_k$ would be $-\nabla f(x^k)$ since it is the opposite direction from $\nabla f(x^k)^T$</p>
<p>Then, the new point is updated every step by $x^{k+1} \leftarrow x_k - \alpha \nabla f(x_k)$</p>
<p>This process will continue until $||\nabla f(x_k)|| \leq \epsilon$ where $\epsilon$ is a very small number. This is the point where the gradient is small enough or vanishes. This means that there is no more 'movement'.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># x = np.arange(2, 8, 0.01)</span>
<span class="c1"># def y(x):</span>
<span class="c1">#     return (x-5)**2</span>

<span class="c1"># fig, ax = plt.subplots()</span>
<span class="c1"># ax.plot(x, y(x))</span>
<span class="c1"># ax.set_xlabel("x")</span>
<span class="c1"># ax.set_ylabel("y")</span>
<span class="c1"># ax.fill_between(x, y(x), 9, color='blue', alpha=.1)</span>

<span class="c1"># ax.plot([7], y(7), marker='o', color='red')</span>
<span class="c1"># ax.plot([5], y(5), marker='o', color='black')</span>

<span class="c1"># def line1(x):</span>
<span class="c1">#     return 4*x - 24</span>
<span class="c1"># def line2(x):</span>
<span class="c1">#     return x*0</span>

<span class="c1"># ax.plot(np.arange(6, 8, 0.01), line1(np.arange(6, 8, 0.01)), color='red')</span>
<span class="c1"># ax.plot(x, line2(x), color='black');</span>
<span class="c1"># fig.savefig('my_icons/03_/five.png')</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Notes/images/copied_from_nb/my_icons/03_/five.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The red dot has a negative gradient, and it moves the point towards left. If the point reaches the black dot, the gradient is zero (vanishes), meaning there is no further direction for the point to move, indicating the end of the search.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br>
<br>
<br>
<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-Newton's-method-from-scratch">
<a class="anchor" href="#4.-Newton's-method-from-scratch" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Newton's method from scratch<a class="anchor-link" href="#4.-Newton's-method-from-scratch"> </a>
</h3>
<p>One of the methods of gradient descent is Newton's method.</p>
<p>Let $x^k$ be the current state and consider a second order Taylor approximation of $f(x)$. <br>
$g(x) = f(x^k)+\nabla f(x^k)^T (x-x^k) + \frac{1}{2}(x-x^k)\nabla^2f(x^k)(x-x^k)$</p>
<p>The next step would be to find the point that minimizes the function $g(x)$. That point would be where the first derivative equals to zero.
$\nabla g(x) = \nabla f(x^k)+\nabla^2f(x^k)(x-x^k) = 0$</p>
<p>Solving this equation for $x$, this gives us <br>
$x = x^k - [\nabla^2f(x_k)]^{-1} \nabla f(x^k)$ which will be the new $x^{k+1}$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br>
<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To formulate this into the code, here is the psuedo code.</p>
<p><img src="/Notes/images/copied_from_nb/my_icons/03_/newtons_method.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Parameters">
<a class="anchor" href="#Parameters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parameters<a class="anchor-link" href="#Parameters"> </a>
</h4>
<ul>
<li>$k$ is the indicator for current step </li>
<li>$\epsilon$ is the minimum value for stopping the search</li>
<li>$\alpha$ is the step size. $\alpha$ changes via line search. This means that we want to find a step size that actually leads to minimizing the function value</li>
<li>$\rho$ and $c$ are scalars used to derive $\alpha$ during line search. They are between 0 and 1</li>
</ul>
<h4 id="Code-walkthrough">
<a class="anchor" href="#Code-walkthrough" aria-hidden="true"><span class="octicon octicon-link"></span></a>Code walkthrough<a class="anchor-link" href="#Code-walkthrough"> </a>
</h4>
<ul>
<li>Set the initial values for $\alpha$, $d$, $k$. Initial starting point $x^0$ will be provided via input</li>
<li>The search ends only if the first derivative of $f(x)$ is smaller than $\epsilon$. Here, it is actually the norm value of the vector.</li>
<li>Inside the search, first determine perform line search to find $\alpha$<ul>
<li>If $\alpha$ is found, update $x^{k+1}$ as well as $d^{k+1}$</li>
</ul>
</li>
<li>Continue until vanishing gradient or when it is small enough</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Coding---from-scratch">
<a class="anchor" href="#Coding---from-scratch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Coding - from scratch<a class="anchor-link" href="#Coding---from-scratch"> </a>
</h4>
<p>Lets solve the optimzal minimum value for the following function.

$$y = 100(x_2-x_1^2)^2 + (1-x_1)^2$$
</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_contour_2d</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

    <span class="n">c_plot</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">c_plot</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
              <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'2D contour plot'</span><span class="p">);</span>
    
<span class="k">def</span> <span class="nf">plot_contour_3d</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">'3d'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">contour3D</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'binary'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"x1"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"x2"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">"y"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'3D contour plot'</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># X1 = np.arange(-50, 50, 0.1)</span>
<span class="c1"># X2 = np.arange(-50, 20, 0.1)</span>
<span class="c1"># X1, X2 = np.meshgrid(X1, X2)</span>
<span class="c1"># y = 100 * (X2 - X1**2)**2 + (1-X1)**2</span>

<span class="c1"># plot_contour_3d(X1, X2, y, 'my_icons/03_/3D_contour.png')</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Notes/images/copied_from_nb/my_icons/03_/3D_contour.png" alt=""></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># define x and y</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">Symbol</span><span class="p">(</span><span class="s1">'x1'</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">Symbol</span><span class="p">(</span><span class="s1">'x2'</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Function Y"</span><span class="p">)</span>
<span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Function Y
</pre>
</div>
</div>

<div class="output_area">



<div class="output_latex output_subarea output_execute_result">
$\displaystyle \left(1 - x_{1}\right)^{2} + 100 \left(- x_{1}^{2} + x_{2}\right)^{2}$
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># in order for the function to be calculated with x input, it has to be 'lambdafied'</span>
<span class="c1"># for more information, see the package sympy</span>
<span class="c1"># it allows for place holders for the function, making it easy for us to read and understand</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x1_value</span><span class="p">,</span> <span class="n">x2_value</span><span class="p">,</span> <span class="n">x1</span><span class="o">=</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="o">=</span><span class="n">x2</span><span class="p">,</span> <span class="n">function</span><span class="o">=</span><span class="n">y</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Input the values for x1 and x2, to the function</span>
<span class="sd">    """</span>
    <span class="n">working_function</span> <span class="o">=</span> <span class="n">lambdify</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">),</span> <span class="n">function</span><span class="p">,</span> <span class="s1">'numpy'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">working_function</span><span class="p">(</span><span class="n">x1_value</span><span class="p">,</span> <span class="n">x2_value</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">hessian_matrix</span><span class="p">(</span><span class="n">x1_value</span><span class="p">,</span> <span class="n">x2_value</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculates the hessian matrix for 2 variable function.</span>
<span class="sd">    """</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">der1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">der2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">]):</span>
            <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1_value</span><span class="p">,</span> <span class="n">x2_value</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">der1</span><span class="p">)</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">der2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">matrix</span>

<span class="k">def</span> <span class="nf">first_order_matrix</span><span class="p">(</span><span class="n">x1_value</span><span class="p">,</span> <span class="n">x2_value</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">der</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">]):</span>
        <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1_value</span><span class="p">,</span> <span class="n">x2_value</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">der</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">matrix</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Partial derivative x1"</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Partial derivative x1
</pre>
</div>
</div>

<div class="output_area">



<div class="output_latex output_subarea output_execute_result">
$\displaystyle - 400 x_{1} \left(- x_{1}^{2} + x_{2}\right) + 2 x_{1} - 2$
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Partial derivative x2"</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Partial derivative x2
</pre>
</div>
</div>

<div class="output_area">



<div class="output_latex output_subarea output_execute_result">
$\displaystyle - 200 x_{1}^{2} + 200 x_{2}$
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Partial derivative x1, x2"</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Partial derivative x1, x2
</pre>
</div>
</div>

<div class="output_area">



<div class="output_latex output_subarea output_execute_result">
$\displaystyle - 400 x_{1}$
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Partial derivative x2, x1"</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Partial derivative x2, x1
</pre>
</div>
</div>

<div class="output_area">



<div class="output_latex output_subarea output_execute_result">
$\displaystyle - 400 x_{1}$
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Newton's method implemented</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">newton_iteration</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    x: initial value</span>
<span class="sd">    """</span>
    <span class="c1"># initialization</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span>
    <span class="c1"># calculate hessian matrix and first derivative matrix</span>
    <span class="n">hessian_m</span> <span class="o">=</span> <span class="n">hessian_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">first_order_m</span> <span class="o">=</span> <span class="n">first_order_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># use them to calculate the gradient</span>
    <span class="n">d</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">hessian_m</span><span class="p">),</span> <span class="n">first_order_m</span><span class="p">)</span>
    <span class="c1"># calculate the l2 norm of gradient. used as a stopping criteria</span>
    <span class="n">gradient_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">first_order_m</span><span class="p">)</span>

    <span class="c1"># initial value for alpha and parameters for line search</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="mf">0.2</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mf">0.4</span>
        
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Initialized algorithm."</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"step: </span><span class="si">{}</span><span class="se">\n</span><span class="s2">d: </span><span class="si">{}</span><span class="se">\n</span><span class="s2">gradient: </span><span class="si">{}</span><span class="se">\n</span><span class="s2">threshold: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">gradient_norm</span><span class="p">,</span> <span class="n">e</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span><span class="o">*</span><span class="mi">40</span><span class="p">)</span>
    
    <span class="c1"># run until gradient is small enough</span>
    <span class="k">while</span> <span class="n">gradient_norm</span> <span class="o">&gt;</span> <span class="n">e</span><span class="p">:</span>
        <span class="c1"># perform line search -&gt; find appropriate alpha</span>
        <span class="k">while</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">c</span><span class="o">*</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">first_order_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
            <span class="n">alpha</span> <span class="o">*=</span> <span class="n">rho</span>
        <span class="c1"># update x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">d</span>
        <span class="c1"># calculate updated hessian and first order matrix</span>
        <span class="n">hessian_m</span> <span class="o">=</span> <span class="n">hessian_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">first_order_m</span> <span class="o">=</span> <span class="n">first_order_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># update d</span>
        <span class="n">d</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">hessian_m</span><span class="p">),</span> <span class="n">first_order_m</span><span class="p">)</span>
        <span class="c1"># update norm value of gradient</span>
        <span class="n">gradient_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">first_order_m</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">function_value</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"step: </span><span class="si">{}</span><span class="se">\n</span><span class="s2">alpha: </span><span class="si">{}</span><span class="se">\n</span><span class="s2">x: </span><span class="si">{}</span><span class="se">\n</span><span class="s2">updated function value: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">function_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"d: </span><span class="si">{}</span><span class="se">\n</span><span class="s2">gradient norm: </span><span class="si">{}</span><span class="se">\n</span><span class="s2">threshold: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">gradient_norm</span><span class="p">,</span> <span class="n">e</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">k</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">newton_iteration</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.1</span><span class="p">],[</span><span class="mf">1.1</span><span class="p">]]),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Function converged at step </span><span class="si">{}</span><span class="s2">. </span><span class="se">\n</span><span class="s2">x = </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Function converged at step 44. 
x = [[1.00000816]
 [1.00001616]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If the starting point is $[x1,x2]^T=[1.2, 1.2]^T$, the function converges to $[x1,x2]^T\approx[1, 1]^T$ at step 44.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">newton_iteration</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">6</span><span class="p">],[</span><span class="mi">4</span><span class="p">]]),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Function converged at step </span><span class="si">{}</span><span class="s2">. </span><span class="se">\n</span><span class="s2">x = </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Function converged at step 491. 
x = [[1.00001367]
 [1.00002717]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If the starting point is $[6, 4]^T$, it takes more iterations (491 steps), but the $x$ value is similar $[x1,x2]^T\approx[1, 1]^T$</p>
<p>Since the gradient is an iterative process, and optimizing a function get exponentially hard as the function gets complicated, the exact number would differ depending on where the iteration starts. That is also why there are local and global minimuns.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br>
<br>
The gradient descent process is printed out for your reference. You can see the gradient decreasing as the function value decreases, while $x$ converges towards 1</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">newton_iteration</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.1</span><span class="p">],[</span><span class="mf">1.1</span><span class="p">]]),</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Initialized algorithm.
step: 0
d: [[-0.00434783]
 [ 0.10043478]]
gradient: 53.34753977457635
threshold: 0.0001
----------------------------------------
step: 1
alpha: 1
x: [[1.09565217]
 [1.20043478]]
updated function value: 0.009149374108868819
d: [[-0.0952919 ]
 [-0.20879466]]
gradient norm: 0.19962485729758325
threshold: 0.0001

step: 2
alpha: 0.2
x: [[1.07659379]
 [1.15867585]]
updated function value: 0.0058809236765559915
d: [[-0.07120573]
 [-0.15294095]]
gradient norm: 0.3250473248156841
threshold: 0.0001

step: 3
alpha: 0.2
x: [[1.06235265]
 [1.12808766]]
updated function value: 0.003913404243300558
d: [[-0.05662774]
 [-0.11981177]]
gradient norm: 0.35423959760700396
threshold: 0.0001

step: 4
alpha: 0.2
x: [[1.0510271 ]
 [1.10412531]]
updated function value: 0.0026321371824487164
d: [[-0.04611446]
 [-0.09640244]]
gradient norm: 0.3429543838077912
threshold: 0.0001

step: 5
alpha: 0.2
x: [[1.04180421]
 [1.08484482]]
updated function value: 0.0017737229394667728
d: [[-0.03792668]
 [-0.07851316]]
gradient norm: 0.3137556000113143
threshold: 0.0001

step: 6
alpha: 0.2
x: [[1.03421887]
 [1.06914219]]
updated function value: 0.0011926921395593723
d: [[-0.03129878]
 [-0.06427308]]
gradient norm: 0.27756728506417017
threshold: 0.0001

step: 7
alpha: 0.2
x: [[1.02795912]
 [1.05628757]]
updated function value: 0.000798717397382519
d: [[-0.02582888]
 [-0.0526897 ]]
gradient norm: 0.2400900376255633
threshold: 0.0001

step: 8
alpha: 0.2
x: [[1.02279334]
 [1.04574963]]
updated function value: 0.0005322515681819315
d: [[-0.021276  ]
 [-0.04316532]]
gradient norm: 0.20432202337886463
threshold: 0.0001

step: 9
alpha: 0.2
x: [[1.01853814]
 [1.03711657]]
updated function value: 0.0003528662051710774
d: [[-0.01747768]
 [-0.0353    ]]
gradient norm: 0.17175012620431226
threshold: 0.0001

step: 10
alpha: 0.2
x: [[1.0150426 ]
 [1.03005657]]
updated function value: 0.0002327782263645213
d: [[-0.01431288]
 [-0.02880145]]
gradient norm: 0.14298481269502508
threshold: 0.0001

step: 11
alpha: 0.2
x: [[1.01218003]
 [1.02429628]]
updated function value: 0.0001528529201920002
d: [[-0.01168431]
 [-0.02344112]]
gradient norm: 0.11812679939146666
threshold: 0.0001

step: 12
alpha: 0.2
x: [[1.00984316]
 [1.01960805]]
updated function value: 9.995613546068292e-05
d: [[-0.00951   ]
 [-0.01903206]]
gradient norm: 0.09698963848128284
threshold: 0.0001

step: 13
alpha: 0.2
x: [[1.00794116]
 [1.01580164]]
updated function value: 6.512846188240299e-05
d: [[-0.00771924]
 [-0.01541733]]
gradient norm: 0.07923808656277329
threshold: 0.0001

step: 14
alpha: 0.2
x: [[1.00639732]
 [1.01271818]]
updated function value: 4.2303526271417644e-05
d: [[-0.00625057]
 [-0.01246374]]
gradient norm: 0.06447413557389624
threshold: 0.0001

step: 15
alpha: 0.2
x: [[1.0051472 ]
 [1.01022543]]
updated function value: 2.7405114468215976e-05
d: [[-0.00505076]
 [-0.01005805]]
gradient norm: 0.05228928155837711
threshold: 0.0001

step: 16
alpha: 0.2
x: [[1.00413705]
 [1.00821382]]
updated function value: 1.7714179820000433e-05
d: [[-0.00407399]
 [-0.00810429]]
gradient norm: 0.042294696053979584
threshold: 0.0001

step: 17
alpha: 0.2
x: [[1.00332225]
 [1.00659296]]
updated function value: 1.1428983950133293e-05
d: [[-0.00328118]
 [-0.00652159]]
gradient norm: 0.03413701016380426
threshold: 0.0001

step: 18
alpha: 0.2
x: [[1.00266601]
 [1.00528864]]
updated function value: 7.362607422700623e-06
d: [[-0.00263936]
 [-0.0052423 ]]
gradient norm: 0.027504938021446976
threshold: 0.0001

step: 19
alpha: 0.2
x: [[1.00213814]
 [1.00424018]]
updated function value: 4.737096082738663e-06
d: [[-0.00212089]
 [-0.00421017]]
gradient norm: 0.022130294612251602
threshold: 0.0001

step: 20
alpha: 0.2
x: [[1.00171396]
 [1.00339815]]
updated function value: 3.0447323568498954e-06
d: [[-0.00170282]
 [-0.00337876]]
gradient norm: 0.017785795014394316
threshold: 0.0001

step: 21
alpha: 0.2
x: [[1.0013734]
 [1.0027224]]
updated function value: 1.955353864088912e-06
d: [[-0.00136622]
 [-0.00270989]]
gradient norm: 0.014281198855098794
threshold: 0.0001

step: 22
alpha: 0.2
x: [[1.00110016]
 [1.00218042]]
updated function value: 1.2549004612078538e-06
d: [[-0.00109553]
 [-0.00217237]]
gradient norm: 0.011458789044168765
threshold: 0.0001

step: 23
alpha: 0.2
x: [[1.00088105]
 [1.00174594]]
updated function value: 8.049277112761418e-07
d: [[-0.00087808]
 [-0.00174077]]
gradient norm: 0.009188780777784835
threshold: 0.0001

step: 24
alpha: 0.2
x: [[1.00070544]
 [1.00139779]]
updated function value: 5.16076208111751e-07
d: [[-0.00070352]
 [-0.00139446]]
gradient norm: 0.00736499520209676
threshold: 0.0001

step: 25
alpha: 0.2
x: [[1.00056473]
 [1.0011189 ]]
updated function value: 3.307632542718407e-07
d: [[-0.0005635 ]
 [-0.00111676]]
gradient norm: 0.005900963590270291
threshold: 0.0001

step: 26
alpha: 0.2
x: [[1.00045203]
 [1.00089554]]
updated function value: 2.119323136229432e-07
d: [[-0.00045124]
 [-0.00089417]]
gradient norm: 0.00472652355749282
threshold: 0.0001

step: 27
alpha: 0.2
x: [[1.00036178]
 [1.00071671]]
updated function value: 1.3576189126097238e-07
d: [[-0.00036128]
 [-0.00071583]]
gradient norm: 0.0037849075285548606
threshold: 0.0001

step: 28
alpha: 0.2
x: [[1.00028953]
 [1.00057354]]
updated function value: 8.695187002111232e-08
d: [[-0.0002892 ]
 [-0.00057298]]
gradient norm: 0.003030290360051668
threshold: 0.0001

step: 29
alpha: 0.2
x: [[1.00023169]
 [1.00045895]]
updated function value: 5.568216012573998e-08
d: [[-0.00023148]
 [-0.00045859]]
gradient norm: 0.002425747303054435
threshold: 0.0001

step: 30
alpha: 0.2
x: [[1.00018539]
 [1.00036723]]
updated function value: 3.565348531745216e-08
d: [[-0.00018526]
 [-0.000367  ]]
gradient norm: 0.001941568396278841
threshold: 0.0001

step: 31
alpha: 0.2
x: [[1.00014834]
 [1.00029383]]
updated function value: 2.2826895374249215e-08
d: [[-0.00014825]
 [-0.00029368]]
gradient norm: 0.0015538763558939346
threshold: 0.0001

step: 32
alpha: 0.2
x: [[1.00011869]
 [1.00023509]]
updated function value: 1.4613653716375122e-08
d: [[-0.00011863]
 [-0.000235  ]]
gradient norm: 0.001243499182237761
threshold: 0.0001

step: 33
alpha: 0.2
x: [[1.00009496]
 [1.00018809]]
updated function value: 9.355013775390774e-09
d: [[-9.49261392e-05]
 [-1.88033408e-04]]
gradient norm: 0.0009950542554677025
threshold: 0.0001

step: 34
alpha: 0.2
x: [[1.00007598]
 [1.00015049]]
updated function value: 5.9883745449197485e-09
d: [[-7.59534568e-05]
 [-1.50448575e-04]]
gradient norm: 0.0007962066119685916
threshold: 0.0001

step: 35
alpha: 0.2
x: [[1.00006079]
 [1.0001204 ]]
updated function value: 3.833156859157997e-09
d: [[-6.07707990e-05]
 [-1.20372852e-04]]
gradient norm: 0.000637069775853806
threshold: 0.0001

step: 36
alpha: 0.2
x: [[1.00004863]
 [1.00009632]]
updated function value: 2.4535262533249835e-09
d: [[-4.86217830e-05]
 [-9.63072398e-05]]
gradient norm: 0.0005097227089664632
threshold: 0.0001

step: 37
alpha: 0.2
x: [[1.00003891]
 [1.00007706]]
updated function value: 1.5704134538484076e-09
d: [[-3.89007196e-05]
 [-7.70515274e-05]]
gradient norm: 0.0004078209843882173
threshold: 0.0001

step: 38
alpha: 0.2
x: [[1.00003113]
 [1.00006165]]
updated function value: 1.0051448367064757e-09
d: [[-3.11226839e-05]
 [-6.16448937e-05]]
gradient norm: 0.00032628419496853685
threshold: 0.0001

step: 39
alpha: 0.2
x: [[1.0000249 ]
 [1.00004932]]
updated function value: 6.433337797516271e-10
d: [[-2.48994967e-05]
 [-4.93182655e-05]]
gradient norm: 0.0002610448991252154
threshold: 0.0001

step: 40
alpha: 0.2
x: [[1.00001992]
 [1.00003946]]
updated function value: 4.1175465760644115e-10
d: [[-1.99204612e-05]
 [-3.94561170e-05]]
gradient norm: 0.0002088471481323487
threshold: 0.0001

step: 41
alpha: 0.2
x: [[1.00001594]
 [1.00003157]]
updated function value: 2.635337540940221e-10
d: [[-1.59369220e-05]
 [-3.15658567e-05]]
gradient norm: 0.0001670849055650914
threshold: 0.0001

step: 42
alpha: 0.2
x: [[1.00001275]
 [1.00002525]]
updated function value: 1.6866711905022221e-10
d: [[-1.27498915e-05]
 [-2.52533019e-05]]
gradient norm: 0.0001336725245054508
threshold: 0.0001

step: 43
alpha: 0.2
x: [[1.0000102]
 [1.0000202]]
updated function value: 1.0794978077835293e-10
d: [[-1.02001398e-05]
 [-2.02030360e-05]]
gradient norm: 0.00010694096373611484
threshold: 0.0001

step: 44
alpha: 0.2
x: [[1.00000816]
 [1.00001616]]
updated function value: 6.908930602668813e-11
d: [[-8.16025679e-06]
 [-1.61626813e-05]]
gradient norm: 8.555465541454276e-05
threshold: 0.0001

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p>END</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="hooman34/Notes"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Notes/optimization/gradient%20descent/2022/02/12/Mathematical_intution_of_gradient_descent.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>All the learnings, distilled here.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" target="_blank" title="fastai"><svg class="svg-icon grey"><use xlink:href="/Notes/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" target="_blank" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/Notes/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
